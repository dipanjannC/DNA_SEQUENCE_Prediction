Objective 

Goal: Predict CpG count in 128-nucleotide DNA sequences.
Data: Randomly generated sequences, labeled by actual CpG counts.
Model: LSTM network with a fully connected output layer and ReLU activation.
Training: Adam optimizer, MSE loss, Optuna hyperparameter tuning (hidden size, layers, dropout, learning rate), and basic Early Stopping on training loss.



1. How you arrived at this approach and why you believe it is the most suitable for this task.

Approach and Rationale:
This problem involves regression on sequential data, and the approach was designed to address this task effectively.

Key Steps: 
1. Understanding the data: Analyzing the data characteristics helped define input-output relationships and identify trends critical for regression.
2. Baseline model: A simple linear regression model was created to set a benchmark for improvement.
3. Choosing loss and optimizer functions: MSE was selected as the loss function due to its suitability for regression, and Adam optimizer was chosen for its adaptive learning capabilities.
4. Hyperparameter Tuning: Optuna was employed to systematically explore hyperparameters for the LSTM model.
5. Evaluation: Metrics such as MSE, MAE, and R-squared were calculated to assess performance.

Implementation:
This approach was chosen for its ability to leverage LSTMs, which are highly effective for sequential data due to their capability to capture long-term dependencies.
Information is managed through a mechanism called cell states, allowing LSTMs to selectively remember or forget things.

A basic LSTM network was created, combined with a fully connected layer for regression, and a ReLU activation for positive output.

Hyperparameter Tuning with Optuna:
Given the number of hyperparameters involved with an LSTM, it makes sense to use Optuna.
The goal was to find optimal settings for hidden_size, num_layers, dropout, and learning_rate to maximize model performance based on the MSE loss.

Evaluation:
MSE, MAE, R-squared on training and test data, results suggest overfitting.


2. Any alternative methods you considered and why you decided against them.

Thoughts and Implementation : 

- Simple LSTM Network: The initial approach used a simple LSTM network than an extensive network.
- ReLU Activation: ReLU was used in the final layer to ensure non-negative predictions as the output ranges (0,inf)
- PyTorch Padding: Utilizing PyTorch's built-in padding functions streamlined data processing than custom padding. 
- Hyperparameter Tuning for Variable Length DNA Sequence: Comprehensive hyperparameter tuning is essential, but a time-intensive process.
- Retraining for Variable Lengths: Retraining was done as the input sequences changed for Variable Length DNA Sequences.
- Padding Value Update: Padding value was changed to 1000 to avoid interfering with training with variable-length inputs.
- Custom Layer Addition: Planned addition of custom layers like LayerNormalization and etc was omitted due to time constraints and could be explored in further iterations.

Major Reasons:
- Time : Due to time constraints, some of the more time-intensive evaluation into alternatives and more comprehensive hyperparameter tuning.


